# 架构师笔记02


## 高可用设计手段

### 什么是高可用

	7* 24小时不间断服务，任何人、时间、地点，任何服务，都能获得预期结果
### 为什么要高可用

硬件

* 有生命周期，会发生故障、会老化。
* 网络抖动或划分

软件

* bugs
* 性能极限
* 软件间相互影响

### 传统高可用评估方式

一段时间（例如一年）的停机时间占比   

停机时间/总运行时间

* 1个9 90% 停机时间不超过880小时/年
* 2个9 停机时间不超过88小时/年
* 3个9 停机时间不超过9小时/年
* 4个9 停机时间不超过53分钟/年
* 5个9 停机时间不超过6分钟/年

### 科学可用评估方式

一段时间（例如一年）停机影响的请求量占比

停机时间影响请求量/总的请求量


### 微服务高可用设计手段

* 服务冗余  --- 无状态化
* 负责均衡  --- 幂等设计
* 超时机制  --- 异步化设计
* 服务限流降级熔断 --- 数据复制/缓存/Sharding
* 架构拆分  --- 服务治理


### 服务实时监控

![服务实时监控](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7.png)

基于日志进行分析，日志内容包含请求耗时以及状态，通过日志收集，以及流计算程序来进行实时计算和监控。还可以沉淀到时序数据库，进行综合环比分析


### 服务分级

![服务分级](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%9C%8D%E5%8A%A1%E5%88%86%E7%BA%A7.png)
	
从技术指标和业务指标两个维度来进行分级

### 高可用案例

如何无缝停止线上服务：

* 网关层已具备热切换能力
	* 热开关切换

网关判断最大超时时间，超过最大超时时间即可保证已经接入的请求完成处理。

进程的话可以发送进程终止消息

自动发现和注册的网关，可以主动发送下线信息

* 网关层不具备热切换能力
	* 防火墙限制只出不进
	* IPTABLES 

### 系统性能

* 吞吐量
* 响应延迟

### 性能优化目标

* 缩短响应时间
* 提高并发数（增加吞吐量）
* 让系统处于合理状态

### 性能优化手段

- 空间换时间

	* 系统时间是瓶颈

例如：缓存复用计算结果，减低时间开销，因为CPU时间较内存容量更加昂贵

- 时间换空间

	* 数据大小是瓶颈
例如：网络传输是瓶颈，使用系统时间缓存传输的数据量，使用HTTP的gzip压缩算法

App的请求接口变化不频繁的数据，使用版本号判断哪些数据被更新，只下载有变更的数据
	
- 找到系统瓶颈

	* 分析系统业务流程，找到关键路径并分解优化。例如： 一个服务集群4W的QPS， 调用量前5的接口贡献3.5W的QPS，那么优化要集中在这前5的接口。
	* 对关键路径的代码优化收益最大，但是系统其它部分也不能忽视
	* 优化层次，从整体到细节，从全局角度到局部视角

### 优化层次

- 架构设计层次
	* 关注系统控制、数据流程
	* 如何拆分系统，如何使各部分系统整体负载更加均衡，充分发挥硬件设施性能优势，检索系统内部开销等

- 算法逻辑层次
	* 关注算法选择是否高效，算法逻辑优化，空间时间优化任务并处理，使用无锁数据结构等
	* 空间换时间  ThreadLocal
	* 时间换空间：采用压缩算法压缩数据，更复杂的逻辑减少数据传输
	* 
- 代码优化层次
	* 关注代码细节优化，代码实现是否合理，是否创建了过多的对象，循环遍历是否高效，cache使用的是否合理，是否重用计算结果等

### 代码优化层次

- 循环遍历是否高效，不要再循环里调RPC接口、查询分布式缓存、执行SQL等
	* 先调用批量接口组装好数据，再循环处理
- 代码逻辑避免生成过多对象或无效对象
	* 输出Log时候先判断Log级别，避免NEW无效对象
```
if(Log.isDebugEnable()){
	Log.debug("debug log");
}
``` 
- ArrayList、HashMap初始容量设置是否合理
	* 扩容是有代价的

- 对数据对象是否合理重用，比如通过RPC查到的数据能复用则必须复用
- 根据数据访问特性选择合适数据结构，比如读多写少，考虑CopyOnWriteArrayList(写时Copy副本)
- 拼接字符串的时候不使用 ”+“ 连接，而是使用StringBuilder进行append
- 是否正确初始化数据，有些全局共享的数据，饿汉式模式，在用户访问之前先初始化好


![代码优化层次1](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E5%B1%82%E6%AC%A1%E6%A1%88%E4%BE%8B1.png)


![代码优化层次2](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E5%B1%82%E6%AC%A1%E6%A1%88%E4%BE%8B2.png)

- 扩大到一般厂家，业务系统使用缓存降低响应时间提高性能，必须提高缓存命中率
- 聚焦的高频访问，时效性要求不高，很适合利用缓存提升性能。例如banner、广告位
- 如果对数据实时性要求很高，比如严格的时效性，需要慎重考虑更新缓存带来的一致性问题
- 时效性和缓存的冲突

商品服务对商品进行了缓存，由于更新缓存和更新商品不是同一个事务，则对数据时效性要求高的业务，只能从数据库查询




### 数据库优化层次
- 数据库建表字段使用尽量小的数据结构
	* 表示状态的字段，如果状态值在255以内使用tinyint
- 使用enum的场景使用tinyint替代，enum扩展需要该表
- 避免使用select * 查询数据，只查询需要的字段，避免浪费数据IO、内存、CPU资源
- 分析查询场景简历合适的所有，分析字段的可选择性，索引长度，对长的varchar使用前缀索引
- 字段经理为Not Null类型，Mysql手册说明允许null的字段需要额外的存储空间去处理null，并且很难查询优化
- 目的为了降低服务器的CPU使用率、IO流量、内存占用、网络消耗，降低响应时间

![数据库优化层次1](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96%E5%B1%82%E6%AC%A11.png)

### 算法逻辑优化层次

- 用更高效的算法替换现有算法，而不改变接口
- 增量式算法，复用之前的计算结果。比如报表服务，要从全量数据中生成报表数据量很大，但是每次增量的数据较少，则可以考虑只计算增量数据的统计结果，和之前的计算结果合并，这样处理的数据量就小很多
- 并发和锁的优化，读多写少的业务场景下，基于CAS的LockFree比mutex性能更好
- 当系统时间是瓶颈,采取空间换时间逻辑算法，分配更多空间节省系统时间
	* 缓存复用计算结果，降低时间开销，CPU时间较内存容量更加昂贵
- 当系统空间容量是瓶颈，采取时间换空间算法策略
	* 网络传输是瓶颈，使用压缩数据大小，提高传输效率
	* APP请求分类接口，使用版本来更新变化的数据
- 并行执行，比如一段逻辑调用了多个RPC接口，而这些接口之间并没有数据依赖，则可以考虑并行调用，降低响应时间
- 异步执行，分析业务流程中的主次流程，把次要流程拆分出来异步执行，更进一步可以拆分到单独的模块去执行，比如使用消息队列，彻底和核心流程解耦，提高核心流程的稳定性以及降低响应时间。

### 架构设计优化层次
- 分布式系统微服务化
- 分库分表，读写分离，数据分片
- 无状态化设计，动态水平弹性扩展
- 调用链路梳理，热点数据尽量靠近用户
- 分布式Cache、多级多类型缓存
- 容量规划
- 提前拒绝，保证柔性可用

### 秒杀系统架构思路
- 数据分层次校验，上层尽量把无效请求过滤掉。秒杀场景里面，99%的请求都是无效的
- 上层可用是不精确的过滤
- 层层限流，最后一层做数据一致性校验，扣减库存

秒杀系统架构设计：

- HTML、JS、CSS等静态资源放CDN，并配置客户端缓存策略
- 非实时动态数据（例如商品标题、描述、图片信息、活动信息等），这些数据缓存在用户访问链路中靠近用户的位置
- 粗过滤一部分流量。比如访问有效性、用户是否有秒杀资格、秒杀是否已经结束等
- 实时数据比如用户营销数据（如红包、折扣）、商品库存等，再次过滤一批客户
- 经过多层过滤最终落到数据库的流量已经较少，最终在数据库层面使用事务保障扣减库存准确性

### Feed系统架构思路

- 读多写少，且比例悬殊。100：1
- 冷热数据明显。80%是当天数据，20%的用户是活跃用户
- 热点效应明显。热点事件、重大节日会引起流量聚集
- 超高访问量

Feed系统架构设计:

- 读多写少、冷热数据明显，热点数据缓存到调用链路更靠近用户的地方
- 多级缓存，L1缓存容量小负责抗最热点的数据，L2缓存容量大，缓存更大范围的数据
- 分类缓存，高热点数据单独缓存。
	* 大V的用户数据单独缓存（利用分流）
	* Feed（关注的feed、topic的feed、一些运营的feed）前几页访问比例超高，针对这种业务特性，做单独缓存处理

![](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/%E5%88%86%E7%BA%A7%E7%BC%93%E5%AD%98.png)	

Feed系统消息发布：

- 基于写扩散消息统一推送通道
- 推送策略： 拆分数据并行推，活跃用户先推，非活跃用户慢慢推
	* 有1W个用户关注，发了一个feed，拆分成100份，每份100个并行推
	* 1W个用户里面活跃的可能2000个，活跃用户先推，非活跃用户慢慢推。保证活跃用户体验，非活跃用户推了很大概率也不看

- 推拉结合。朋友圈的小红点和新消息条数是推的；进入朋友圈获取内容是拉的

![](https://github.com/richenlin/DayDayUp/raw/master/%E6%9E%B6%E6%9E%84%E5%B8%88/Feed%E7%B3%BB%E7%BB%9F.png)

Feed系统存储选型：

数据类型 | 特点 |  存储解决方案 | 存储产品
------------- | ------------- | ------------- | -------------
微博内容 | 类型简单、海量访问 | 关系型数据库、KV存储 | MySQL、TiDB、Pika
微博列表 | 结构化列表数据、多维度查询 | 关系型数据库 | MySQL、TiDB
关系 | 类型简单、高速访问 | 持久化KV存储 | Redis、Pika
长微博（图片、短视频）| 对象数据（小文件等） | 对象存储 | Ceph、 MooseFS
计数（关注数、粉丝数）| 结构简单、数据及访问量大 | 内存KV存储 | Redis



		
* 高并发设计手段
* 服务无状态化设计
* 服务负载均衡设计
* 服务幂等设计
* 分布式锁设计
* 分布式事务设计
* 服务降级设计
* 服务限流/熔断设计
* 服务灰度发布设计
* 服务全链路压测设计
* 亿级真实案例实战